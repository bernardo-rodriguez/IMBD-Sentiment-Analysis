{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn import linear_model\n",
    "from sklearn import tree\n",
    "from sklearn import model_selection\n",
    "from sklearn import ensemble\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def segmentWords(s): \n",
    "    return s.split()\n",
    "\n",
    "def readFile(fileName):\n",
    "    # Function for reading file\n",
    "    # input: filename as string\n",
    "    # output: contents of file as list containing single words\n",
    "    contents = []\n",
    "    f = open(fileName)\n",
    "    for line in f:\n",
    "        contents.append(line)\n",
    "    f.close()\n",
    "    result = segmentWords('\\n'.join(contents))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a Dataframe containing the counts of each word in a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d = []\n",
    "\n",
    "for c in os.listdir(\"data_training\"):\n",
    "    directory = \"data_training/\" + c\n",
    "    for file in os.listdir(directory):\n",
    "        words = readFile(directory + \"/\" + file)\n",
    "        e = {x:words.count(x) for x in words}\n",
    "        e['__FileID__'] = file\n",
    "        e['__CLASS__'] = 'positive' if c=='pos' else 'negative'\n",
    "        d.append(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dataframe from d - make sure to fill all the nan values with zeros.\n",
    "\n",
    "References:\n",
    "\n",
    "https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html\n",
    "https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.fillna.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(d)\n",
    "df.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split data into training and validation set "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "* Sample 80% of your dataframe to be the training data\n",
    "\n",
    "* Let the remaining 20% be the validation data (you can filter out the indicies of the original dataframe that weren't selected for the training data)\n",
    "\n",
    "References:\n",
    "\n",
    "https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.sample.html\n",
    "https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.drop.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = df.sample(frac=0.8, random_state=42)\n",
    "test = df.drop(list(train.index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Split the dataframe for both training and validation data into x and y dataframes - where y contains the labels and x contains the words\n",
    "\n",
    "References:\n",
    "\n",
    "https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.drop.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train = train[['__CLASS__']]\n",
    "y_test = test[['__CLASS__']]\n",
    "X_train = train.drop(['__CLASS__', '__FileID__'], axis=1)\n",
    "X_test = test.drop(['__CLASS__', '__FileID__'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic Logistic Regression\n",
    "* Use sklearn's linear_model.LogisticRegression() to create your model.\n",
    "* Fit the data and labels with your model.\n",
    "* Score your model with the same data and labels.\n",
    "\n",
    "References:\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jun Seo\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:547: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 1.0\n",
      "Test score: 0.796875\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "print(\"Train score: \" + str(model.score(X_train, y_train)))\n",
    "print(\"Test score: \" + str(model.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Changing Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jun Seo\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:547: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C=0.001, penalty=l1:     Train score = 0.490625,  Test score = 0.540625\n",
      "C=0.005, penalty=l1:     Train score = 0.639844,  Test score = 0.640625\n",
      "C=0.010, penalty=l1:     Train score = 0.692969,  Test score = 0.671875\n",
      "C=0.050, penalty=l1:     Train score = 0.844531,  Test score = 0.765625\n",
      "C=1.000, penalty=l1:     Train score = 1.000000,  Test score = 0.796875\n",
      "C=0.001, penalty=l2:     Train score = 0.878125,  Test score = 0.762500\n",
      "C=0.005, penalty=l2:     Train score = 0.977344,  Test score = 0.809375\n",
      "C=0.010, penalty=l2:     Train score = 0.989844,  Test score = 0.806250\n",
      "C=0.050, penalty=l2:     Train score = 1.000000,  Test score = 0.812500\n",
      "C=1.000, penalty=l2:     Train score = 1.000000,  Test score = 0.796875\n"
     ]
    }
   ],
   "source": [
    "for p in ['l1', 'l2']:\n",
    "    for c in [0.001, 0.005, 0.01, 0.05, 1]:\n",
    "        model2 = LogisticRegression(C=c, penalty=p)\n",
    "        model2.fit(X_train, y_train)\n",
    "        print(\"C=\" + str(format(c, '.3f')) + \", penalty=\" + str(p) + \":     Train score = \" + \n",
    "              str(format(model2.score(X_train, y_train), '.6f'))\n",
    "             + \",  Test score = \" + str(format(model2.score(X_test, y_test), '.6f')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Selection\n",
    "* In the backward stepsize selection method, you can remove coefficients and the corresponding x columns, where the coefficient is more than a particular amount away from the mean - you can choose how far from the mean is reasonable.\n",
    "\n",
    "References:\n",
    "\n",
    "https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html#\n",
    "https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.sample.html\n",
    "https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.drop.html\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.where.html\n",
    "https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.std.html\n",
    "https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.mean.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C=0.0010, penalty=l1:     Train score = 0.490625,  Test score = 0.540625\n",
      "C=0.0050, penalty=l1:     Train score = 0.631250,  Test score = 0.600000\n",
      "C=0.0100, penalty=l1:     Train score = 0.683594,  Test score = 0.671875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jun Seo\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:547: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C=0.0500, penalty=l1:     Train score = 0.825781,  Test score = 0.787500\n",
      "C=0.1000, penalty=l1:     Train score = 0.894531,  Test score = 0.778125\n",
      "C=0.5000, penalty=l1:     Train score = 0.998437,  Test score = 0.768750\n",
      "C=1.0000, penalty=l1:     Train score = 1.000000,  Test score = 0.775000\n",
      "C=0.0010, penalty=l2:     Train score = 0.860938,  Test score = 0.768750\n",
      "C=0.0050, penalty=l2:     Train score = 0.950000,  Test score = 0.793750\n",
      "C=0.0100, penalty=l2:     Train score = 0.973437,  Test score = 0.796875\n",
      "C=0.0500, penalty=l2:     Train score = 0.999219,  Test score = 0.812500\n",
      "C=0.1000, penalty=l2:     Train score = 1.000000,  Test score = 0.818750\n",
      "C=0.5000, penalty=l2:     Train score = 1.000000,  Test score = 0.825000\n",
      "C=1.0000, penalty=l2:     Train score = 1.000000,  Test score = 0.821875\n"
     ]
    }
   ],
   "source": [
    "weights = (np.array(model.coef_[0])-np.mean(model.coef_[0]))/np.std(model.coef_[0])\n",
    "X_train_bss = X_train.iloc[:, np.where(np.abs(weights)>3)[0].tolist()]\n",
    "X_test_bss = X_test.iloc[:, np.where(np.abs(weights)>3)[0].tolist()]\n",
    "\n",
    "for p in ['l1', 'l2']:\n",
    "    for c in [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1]:\n",
    "        model3 = LogisticRegression(C=c, penalty=p)\n",
    "        model3.fit(X_train_bss, y_train)\n",
    "        #print(\"C= \" + str(c) + \": Train score = \" + str(model3.score(X_train_bss, y_train))\n",
    "        #     + \", Test score = \" + str(model3.score(X_test_bss, y_test)))\n",
    "        print(\"C=\" + str(format(c, '.4f')) + \", penalty=\" + str(p) + \":     Train score = \" + \n",
    "                  str(format(model3.score(X_train_bss, y_train), '.6f'))\n",
    "                 + \",  Test score = \" + str(format(model3.score(X_test_bss, y_test), '.6f')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1280, 1011)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_bss.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How did you select which features to remove? Why did that reduce overfitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removed features based on weight. Based on different C values, c=0.05 and penalty='l1' performed well while decreasing the overfitting on the train set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic Decision Tree\n",
    "\n",
    "* Initialize your model as a decision tree with sklearn.\n",
    "* Fit the data and labels to the model.\n",
    "\n",
    "References:\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dtree = tree.DecisionTreeClassifier(max_depth=40)\n",
    "dtree = dtree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.63125\n"
     ]
    }
   ],
   "source": [
    "print(dtree.score(X_train, y_train))\n",
    "print(dtree.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Changing Parameters\n",
    "* To test out which value is optimal for a particular parameter, you can either loop through various values or look into sklearn.model_selection.GridSearchCV\n",
    "\n",
    "References:\n",
    "\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How did you choose which parameters to change and what value to give to them? Feel free to show a plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c, r = np.shape(y_train)\n",
    "y_train_reshape = np.reshape(y_train, (c,))\n",
    "\n",
    "params = {\"max_depth\": [10,20,30,40,50]}\n",
    "gscv = model_selection.GridSearchCV(dtree, params)\n",
    "gscv = gscv.fit(X_train.values, y_train_reshape.values.reshape((c,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([ 6.59381159,  7.13364633,  6.38297025,  6.33252009,  7.24861908]),\n",
       " 'mean_score_time': array([ 0.0862515 ,  0.07942883,  0.07935596,  0.07526914,  0.08846656]),\n",
       " 'mean_test_score': array([ 0.6234375 ,  0.62578125,  0.6265625 ,  0.628125  ,  0.628125  ]),\n",
       " 'mean_train_score': array([ 0.94216849,  1.        ,  1.        ,  1.        ,  1.        ]),\n",
       " 'param_max_depth': masked_array(data = [10 20 30 40 50],\n",
       "              mask = [False False False False False],\n",
       "        fill_value = ?),\n",
       " 'params': [{'max_depth': 10},\n",
       "  {'max_depth': 20},\n",
       "  {'max_depth': 30},\n",
       "  {'max_depth': 40},\n",
       "  {'max_depth': 50}],\n",
       " 'rank_test_score': array([5, 4, 3, 1, 1]),\n",
       " 'split0_test_score': array([ 0.58411215,  0.60747664,  0.5817757 ,  0.59813084,  0.59813084]),\n",
       " 'split0_train_score': array([ 0.91784038,  1.        ,  1.        ,  1.        ,  1.        ]),\n",
       " 'split1_test_score': array([ 0.6056338 ,  0.5915493 ,  0.61502347,  0.61971831,  0.60328638]),\n",
       " 'split1_train_score': array([ 0.96604215,  1.        ,  1.        ,  1.        ,  1.        ]),\n",
       " 'split2_test_score': array([ 0.68075117,  0.67840376,  0.68309859,  0.66666667,  0.68309859]),\n",
       " 'split2_train_score': array([ 0.94262295,  1.        ,  1.        ,  1.        ,  1.        ]),\n",
       " 'std_fit_time': array([ 0.44146335,  0.3295914 ,  0.16080226,  0.60376174,  0.80793732]),\n",
       " 'std_score_time': array([ 0.00672959,  0.00728223,  0.00738919,  0.00385014,  0.00316092]),\n",
       " 'std_test_score': array([ 0.04142269,  0.03773108,  0.04217583,  0.0286133 ,  0.0388837 ]),\n",
       " 'std_train_score': array([ 0.01968092,  0.        ,  0.        ,  0.        ,  0.        ])}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gscv.cv_results_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why is a single decision tree so prone to overfitting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = [10,20,30,40,50]\n",
    "y = [tree.DecisionTreeClassifier(max_depth=k).fit(X_train,y_train_reshape).score(X_test,y_test) for k in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD8CAYAAABw1c+bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x14b24a43438>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(x,y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A single decision tree can make a lot of branches, and these branches are often only relevant for certain samples. These branches are basically useless because they're not generalized. Therefore, by decreasing the max depth in our classifier, we prevent the creation of these sample-specific branches and prevent our tree from overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic Random Forest\n",
    "\n",
    "* Use sklearn's ensemble.RandomForestClassifier() to create your model.\n",
    "* Fit the data and labels with your model.\n",
    "* Score your model with the same data and labels.\n",
    "\n",
    "References:\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic Random Forest Classifier score for training data is 0.9875\n",
      "Basic Random Forest Classifier score for test data is 0.659375\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier()\n",
    "clf.fit(X_train,y_train.values.ravel())\n",
    "print(\"Basic Random Forest Classifier score for training data is \" + str(clf.score(X_train, y_train.values.ravel())))\n",
    "print(\"Basic Random Forest Classifier score for test data is \" + str(clf.score(X_test, y_test.values.ravel())))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Changing Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Paramters Random Forest Classifier score for training data is 1.0\n",
      "New Parameters Random Forest Classifier score for test data is 0.7875\n"
     ]
    }
   ],
   "source": [
    "clf.set_params(n_estimators = 400, max_features=(int(clf.n_features_ * .25)))\n",
    "clf.fit(X_train,y_train.values.ravel())\n",
    "print(\"New Paramters Random Forest Classifier score for training data is \" + str(clf.score(X_train, y_train.values.ravel())))\n",
    "print(\"New Parameters Random Forest Classifier score for test data is \" + str(clf.score(X_test, y_test.values.ravel())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What parameters did you choose to change and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''We chose to change n_estimators because the more trees in a random forest the more accurate the classifier. \n",
    "We also decided to change the max_features parameter to prevent overfitting and bring down the training\n",
    "time for our random forest'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does a random forest classifier prevent overfitting better than a single decision tree?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''This happens because overfitting often happens because of high amounts of variance. Random forest classifiers \n",
    "inherently have less variance because they average out the variance in single decision trees.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
